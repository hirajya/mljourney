{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Language Modelling with LSTM**\n",
        "Recurrent Networks and LSTM in Deep Learning\n"
      ],
      "metadata": {
        "id": "NKvUk7Uafbkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HxsppUiSfXnS"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
        "!tar xzf simple-examples.tgz -C data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KQi9dCxgZ-l",
        "outputId": "ba1da573-ed3e-4075-899e-c2335d2fe44a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-05 03:03:38--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M   898KB/s    in 61s     \n",
            "\n",
            "2023-06-05 03:04:41 (557 KB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n",
            "tar: data: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial weight scale\n",
        "init_scale = 0.1\n",
        "\n",
        "# Initial learning rate\n",
        "learning_rate = 1.0\n",
        "\n",
        "# Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
        "max_grad_norm = 5\n",
        "\n",
        "# The number of layers in our model\n",
        "num_layers = 2\n",
        "\n",
        "# The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
        "num_steps = 20\n",
        "\n",
        "# The number of processing units (neurons) in the hidden layers\n",
        "hidden_size_l1 = 256\n",
        "hidden_size_l2 = 128\n",
        "\n",
        "# The maximum number of epochs trained with the initial learning rate\n",
        "max_epoch_decay_lr = 4\n",
        "\n",
        "# The total number of epochs in training \n",
        "max_epoch = 15\n",
        "\n",
        "# The probability for keeping data in the Dropout layer (This is an optimization)\n",
        "# At 1, we ignore the dropout layer wrapping\n",
        "keep_prob = 1\n",
        "\n",
        "# The decay for the learning rate\n",
        "decay = 0.5\n",
        "\n",
        "# The size for each batch of data\n",
        "batch_size = 30\n",
        "\n",
        "# The size of our vocabulary\n",
        "vocab_size = 10000\n",
        "embeding_Vector_size = 200\n",
        "\n",
        "# Training flag to seperate training from testing \n",
        "is_training = 1\n",
        "\n",
        "# Data directory for our dataset\n",
        "data_dir = \"/content/lstm/\"\n"
      ],
      "metadata": {
        "id": "CSrfz1gkgpU-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data"
      ],
      "metadata": {
        "id": "NLoEcGvcjOKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reader\n",
        "import reader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7qVGdavj-VK",
        "outputId": "e9275e1f-4da6-45b2-8e2b-354305da4f3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting reader\n",
            "  Downloading reader-3.5-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from reader) (4.5.0)\n",
            "Collecting feedparser>=6 (from reader)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.10/dist-packages (from reader) (2.27.1)\n",
            "Collecting iso8601>=1 (from reader)\n",
            "  Downloading iso8601-1.1.0-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.5 in /usr/local/lib/python3.10/dist-packages (from reader) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.5->reader) (2.4.1)\n",
            "Collecting sgmllib3k (from feedparser>=6->reader)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->reader) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->reader) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->reader) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18->reader) (3.4)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=1cbc1e3147928392687dd3ad23a3e4f200957e16f137f620e00fbe5142e2ee55\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, iso8601, feedparser, reader\n",
            "Successfully installed feedparser-6.0.10 iso8601-1.1.0 reader-3.5 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from reader import make_reader\n",
        "\n",
        "# Reads the data and seperates it into training data, validation data and testing data\n",
        "\n",
        "# train_data = data not available"
      ],
      "metadata": {
        "id": "k-h01iBCi6sk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id_to_word(id_list):\n",
        "  line = []\n",
        "  for w in id_list:\n",
        "    for word, wid in word_to_id.items():\n",
        "      if wid == w:\n",
        "        line.append(word)\n",
        "  return line\n",
        "# print(id_to_word(train_data[0:100]))"
      ],
      "metadata": {
        "id": "HozFkqU6vxR1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
        "# first_touple = itera.__next__()\n",
        "# _input_data = first_touple[0]\n",
        "# _targets = first_touple[1]"
      ],
      "metadata": {
        "id": "jaQJuPcbwI9M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _input_data.shape\n",
        "# _targets.shape\n",
        "# _input_data[0:3]\n",
        "# print(id_to_word(_input_data[0, :]))"
      ],
      "metadata": {
        "id": "wDl_Xi56wfEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_layer = tf.keras.layers.Embedding(vocab_size, embeding_vector_size, batch_input_shape=(batch_size, num_steps), trainable=True, name=\"embedding_vocab\")\n",
        "# inputs = embedding_layer(_input_data)\n",
        "# inputs"
      ],
      "metadata": {
        "id": "h9DwPMmhwuyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "eeD6c2Ilxdeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
        "# lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l2)"
      ],
      "metadata": {
        "id": "CLimrSXNxcfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stacked_lstm = tf.keras.layers.StackedRNNCells([[lstm_cell_l1, lstm_cell_l2]])\n",
        "# layer = tf.keras.layers.RNN(stacked_lstm, [batch_size, num_steps], return_state=False, stateful=True, trainable=True)\n"
      ],
      "metadata": {
        "id": "G9Y-Wrm7xo7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # _initial_state\n",
        "\n",
        "# init_state = tf.Variable(tf.zeros([batch_size, embeding_vector_size]), trainable=False)\n",
        "# layer.inital_state = init_state\n",
        "# layer.inital_state\n",
        "\n",
        "# outputs = layer(inputs)\n",
        "# outputs"
      ],
      "metadata": {
        "id": "tKlUv4vfyHMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dense layer; reshaping our outputs tensor from [30 x 20 x 128] to [30 x 20 x 10000]\n",
        "\n",
        "# dense = tf.keras.layers.Dense(vocab_size)\n",
        "# logits_outputs = dense(outputs)\n",
        "# print(\"shape of the output from dense layer: \", logits_outputs.shape)"
      ],
      "metadata": {
        "id": "67D8pT4gyckg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Activation layer A softmax activation layers is also then applied to derive the probability of the output being in any of the multiclass(10000 in this case) possibilities.\n",
        "\n",
        "# # activation = tf.keras.layers.Activation('softmax')\n",
        "# # output_words_prob = activation(logits_outputs)\n",
        "# # print(\"shape of the output from the activation layer: \", output_words_prob.shape) #(batch_size, sequence_length, vocab_size)\n",
        "# print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0,0:num_steps])"
      ],
      "metadata": {
        "id": "bUQW0cSwy2f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Prediction\n",
        "\n",
        "# np.argmax(output_words_prob[0, 0:num_steps], axis=1)\n",
        "# _targets[0]"
      ],
      "metadata": {
        "id": "93sO6pB5zHmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Objective function</h4>\n",
        "\n",
        "How similar the predicted words are to the target words?\n",
        "\n",
        "Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n",
        "\n",
        "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}\\_i}$$\n",
        "\n",
        "This function is already implemented and available in TensorFlow through _tf.keras.losses.sparse_categorical_crossentropy_. It calculates the categorical cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.  \n",
        "\n",
        "The arguments of this function are:  \n",
        "\n",
        "<ul>\n",
        "    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  \n",
        "    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   \n",
        "</ul>\n"
      ],
      "metadata": {
        "id": "7wUOGbxjzWKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def crossentropy(y_trie, y_pred):\n",
        "#   return tf.keras.losses.sparse_categorical_crossentrophy(y_true, y_pred)\n",
        "\n",
        "# loss = corssentrophy(_targets, output_words_prob)\n",
        "# loss[0:10]\n",
        "\n",
        "# cost = tf.recude_sum(loss / batch_size)\n",
        "# cost"
      ],
      "metadata": {
        "id": "gW6xv8uPzXst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training</h3>\n",
        "\n",
        "To do training for our network, we have to take the following steps:\n",
        "\n",
        "<ol>\n",
        "    <li>Define the optimizer.</li>\n",
        "    <li>Assemble layers to build model.</li>\n",
        "    <li>Calculate the gradients based on the loss function.</li>\n",
        "    <li>Apply the optimizer to the variables/gradients tuple.</li>\n",
        "</ol>\n"
      ],
      "metadata": {
        "id": "bB9oPEcD024F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Define Optimizer"
      ],
      "metadata": {
        "id": "N8bgiG3l077S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # create a variable for the learning rate\n",
        "# lr = tf.Variable(0.0, trainable=False)\n",
        "# optimizer = tf.keras.optimizers.SGD(lr=lr, clipnorm=max_grad_norm)"
      ],
      "metadata": {
        "id": "g5jFhgXe03-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Assemble layers to build model."
      ],
      "metadata": {
        "id": "9bSkJ81y1OE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = tf.keras.Sequential()\n",
        "# model.add(embedding_layer)\n",
        "# model.add(layer)\n",
        "# model.add(dense)\n",
        "# model.add(activation)\n",
        "# model.compile(loss=crossentrophy, optimizer=optimizer)\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "X6yE-mcG1RRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Trainable Variables\n",
        "Defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>.\n"
      ],
      "metadata": {
        "id": "U01GNBix1hho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
        "# tvars = model.trainable_variables\n",
        "# [v.name for v in tvars] "
      ],
      "metadata": {
        "id": "toTPkr2e1keG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Calculate the gradients based on the loss function\n",
        "\n",
        "**Gradient**: The gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by the <b>derivative</b> operation.\n"
      ],
      "metadata": {
        "id": "Xd22XydL1rWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # x = tf.constant(1.0)\n",
        "# # # y = tf.constant(2.0)\n",
        "# # # with tf.GradientTape(persistent=True) as g:\n",
        "# # #   g.watch(x)\n",
        "# # #   g.watch(y)\n",
        "# # #   func_test = 2 * x * x + 3 * x * y\n",
        "\n",
        "# # var_grad = g.gradient(func_test, x)\n",
        "# # print(var_grad)\n",
        "\n",
        "# var_grad = g.gradient(func_test, y)\n",
        "# print(var_grad)\n",
        "\n"
      ],
      "metadata": {
        "id": "c8xB563l2z9v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # with tf.GradientTape() as tape:\n",
        "# #   # forward pass\n",
        "# #   output_words_prob = model(_input_data)\n",
        "\n",
        "# #   # loss value for this batch\n",
        "# #   loss = crossentrophy(_targets, output_words_prob)\n",
        "# #   cost = tf.reduce_sum(loss, axis=0) / batch_size\n",
        "\n",
        "# grad_t_list = tape.gradient(cost, tvars)\n",
        "\n",
        "# print(grad_t_list)\n",
        "\n",
        "# grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
        "# grads"
      ],
      "metadata": {
        "id": "wkxbwZQW4wZ-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Apply the optimizer to the variables/gradients tuple\n"
      ],
      "metadata": {
        "id": "qNB-NiDx5gVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # create the training tensorflow operation through our optimizer\n",
        "# train_op = optimizer.apply_gradients(zip(grads, tvars))"
      ],
      "metadata": {
        "id": "Q3Qclz5m5ftH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM\n",
        "\n",
        "We learned how the model is build step by step. Noe, let's then create a Class that represents our model. This class needs a few things:\n",
        "\n",
        "<ul>\n",
        "    <li>We have to create the model in accordance with our defined hyperparameters</li>\n",
        "    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>\n",
        "    <li>We have to create the word embeddings and point them to the input data</li>\n",
        "    <li>We have to create the input structure for our RNN</li>\n",
        "    <li>We need to create a logistic structure to return the probability of our words</li>\n",
        "    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>\n",
        "    <li>And finally, we need to create a training operation that can be run to actually train our model</li>\n",
        "</ul>\n"
      ],
      "metadata": {
        "id": "HxZod-Mj5wT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class PTBModel(object):\n",
        "\n",
        "\n",
        "#     def __init__(self):\n",
        "#         ######################################\n",
        "#         # Setting parameters for ease of use #\n",
        "#         ######################################\n",
        "#         self.batch_size = batch_size\n",
        "#         self.num_steps = num_steps\n",
        "#         self.hidden_size_l1 = hidden_size_l1\n",
        "#         self.hidden_size_l2 = hidden_size_l2\n",
        "#         self.vocab_size = vocab_size\n",
        "#         self.embeding_vector_size = embeding_vector_size\n",
        "#         # Create a variable for the learning rate\n",
        "#         self._lr = 1.0\n",
        "        \n",
        "#         ###############################################################################\n",
        "#         # Initializing the model using keras Sequential API  #\n",
        "#         ###############################################################################\n",
        "        \n",
        "#         self._model = tf.keras.models.Sequential()\n",
        "        \n",
        "#         ####################################################################\n",
        "#         # Creating the word embeddings layer and adding it to the sequence #\n",
        "#         ####################################################################\n",
        "#         with tf.device(\"/cpu:0\"):\n",
        "#             # Create the embeddings for our input data. Size is hidden size.\n",
        "#             self._embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embeding_vector_size,batch_input_shape=(self.batch_size, self.num_steps),trainable=True,name=\"embedding_vocab\")  #[10000x200]\n",
        "#             self._model.add(self._embedding_layer)\n",
        "            \n",
        "\n",
        "#         ##########################################################################\n",
        "#         # Creating the LSTM cell structure and connect it with the RNN structure #\n",
        "#         ##########################################################################\n",
        "#         # Create the LSTM Cells. \n",
        "#         # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
        "#         # The argument  of LSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A). \n",
        "#         # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
        "#         lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
        "#         lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\n",
        "        \n",
        "\n",
        "        \n",
        "#         # By taking in the LSTM cells as parameters, the StackedRNNCells function junctions the LSTM units to the RNN units.\n",
        "#         # RNN cell composed sequentially of stacked simple cells.\n",
        "#         stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "#         ############################################\n",
        "#         # Creating the input structure for our RNN #\n",
        "#         ############################################\n",
        "#         # Input structure is 20x[30x200]\n",
        "#         # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
        "#         # The input structure is fed from the embeddings, which are filled in by the input data\n",
        "#         # Feeding a batch of b sentences to a RNN:\n",
        "#         # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
        "#         # In step 2,  second word of each of the b sentences is input in parallel. \n",
        "#         # The parallelism is only for efficiency.  \n",
        "#         # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
        "#         # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
        "\n",
        "#         ########################################################################################################\n",
        "#         # Instantiating our RNN model and setting stateful to True to feed forward the state to the next layer #\n",
        "#         ########################################################################################################\n",
        "        \n",
        "#         self._RNNlayer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)\n",
        "        \n",
        "#         # Define the initial state, i.e., the model state for the very first data point\n",
        "#         # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
        "#         self._initial_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\n",
        "#         self._RNNlayer.inital_state = self._initial_state\n",
        "    \n",
        "#         ############################################\n",
        "#         # Adding RNN layer to keras sequential API #\n",
        "#         ############################################        \n",
        "#         self._model.add(self._RNNlayer)\n",
        "        \n",
        "#         #self._model.add(tf.keras.layers.LSTM(hidden_size_l1,return_sequences=True,stateful=True))\n",
        "#         #self._model.add(tf.keras.layers.LSTM(hidden_size_l2,return_sequences=True))\n",
        "        \n",
        "        \n",
        "#         ####################################################################################################\n",
        "#         # Instantiating a Dense layer that connects the output to the vocab_size  and adding layer to model#\n",
        "#         ####################################################################################################\n",
        "#         self._dense = tf.keras.layers.Dense(self.vocab_size)\n",
        "#         self._model.add(self._dense)\n",
        " \n",
        "        \n",
        "#         ####################################################################################################\n",
        "#         # Adding softmax activation layer and deriving probability to each class and adding layer to model #\n",
        "#         ####################################################################################################\n",
        "#         self._activation = tf.keras.layers.Activation('softmax')\n",
        "#         self._model.add(self._activation)\n",
        "\n",
        "#         ##########################################################\n",
        "#         # Instantiating the stochastic gradient decent optimizer #\n",
        "#         ########################################################## \n",
        "#         self._optimizer = tf.keras.optimizers.SGD(lr=self._lr, clipnorm=max_grad_norm)\n",
        "        \n",
        "        \n",
        "#         ##############################################################################\n",
        "#         # Compiling and summarizing the model stacked using the keras sequential API #\n",
        "#         ##############################################################################\n",
        "#         self._model.compile(loss=self.crossentropy, optimizer=self._optimizer)\n",
        "#         self._model.summary()\n",
        "\n",
        "\n",
        "#     def crossentropy(self,y_true, y_pred):\n",
        "#         return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "#     def train_batch(self,_input_data,_targets):\n",
        "#         #################################################\n",
        "#         # Creating the Training Operation for our Model #\n",
        "#         #################################################\n",
        "#         # Create a variable for the learning rate\n",
        "#         self._lr = tf.Variable(0.0, trainable=False)\n",
        "#         # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
        "#         tvars = self._model.trainable_variables\n",
        "#         # Define the gradient clipping threshold\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             # Forward pass.\n",
        "#             output_words_prob = self._model(_input_data)\n",
        "#             # Loss value for this batch.\n",
        "#             loss  = self.crossentropy(_targets, output_words_prob)\n",
        "#             # average across batch and reduce sum\n",
        "#             cost = tf.reduce_sum(loss/ self.batch_size)\n",
        "#         # Get gradients of loss wrt the trainable variables.\n",
        "#         grad_t_list = tape.gradient(cost, tvars)\n",
        "#         # Define the gradient clipping threshold\n",
        "#         grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
        "#         # Create the training TensorFlow Operation through our optimizer\n",
        "#         train_op = self._optimizer.apply_gradients(zip(grads, tvars))\n",
        "#         return cost\n",
        "        \n",
        "#     def test_batch(self,_input_data,_targets):\n",
        "#         #################################################\n",
        "#         # Creating the Testing Operation for our Model #\n",
        "#         #################################################\n",
        "#         output_words_prob = self._model(_input_data)\n",
        "#         loss  = self.crossentropy(_targets, output_words_prob)\n",
        "#         # average across batch and reduce sum\n",
        "#         cost = tf.reduce_sum(loss/ self.batch_size)\n",
        "\n",
        "#         return cost\n",
        "\n",
        "#     def instance(cls) : \n",
        "#         return PTBModel()"
      ],
      "metadata": {
        "id": "tDCUo2Ye6BuR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ########################################################################################################################\n",
        "# # run_one_epoch takes as parameters  the model instance, the data to be fed, training or testing mode and verbose info #\n",
        "# ########################################################################################################################\n",
        "# def run_one_epoch(m, data,is_training=True,verbose=False):\n",
        "\n",
        "#     #Define the epoch size based on the length of the data, batch size and the number of steps\n",
        "#     epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
        "#     start_time = time.time()\n",
        "#     costs = 0.\n",
        "#     iters = 0\n",
        "    \n",
        "#     m._model.reset_states()\n",
        "    \n",
        "#     #For each step and data point\n",
        "#     for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
        "        \n",
        "#         #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
        "#         #y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
        "#         if is_training : \n",
        "#             loss=  m.train_batch(x, y)\n",
        "#         else :\n",
        "#             loss = m.test_batch(x, y)\n",
        "                                   \n",
        "\n",
        "#         #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
        "#         costs += loss\n",
        "        \n",
        "#         #Add number of steps to iteration counter\n",
        "#         iters += m.num_steps\n",
        "\n",
        "#         if verbose and step % (epoch_size // 10) == 10:\n",
        "#             print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
        "        \n",
        "\n",
        "\n",
        "#     # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
        "#     return np.exp(costs / iters)\n"
      ],
      "metadata": {
        "id": "-2QFz7Gp5uQO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Reads the data and separates it into training data, validation data and testing data\n",
        "# raw_data = reader.ptb_raw_data(data_dir)\n",
        "# train_data, valid_data, test_data, _, _ = raw_data"
      ],
      "metadata": {
        "id": "ULT1nXOu55uS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Instantiates the PTBModel class\n",
        "# m=PTBModel.instance()   \n",
        "# K = tf.keras.backend \n",
        "# for i in range(max_epoch):\n",
        "#     # Define the decay for this epoch\n",
        "#     lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
        "#     dcr = learning_rate * lr_decay\n",
        "#     m._lr = dcr\n",
        "#     K.set_value(m._model.optimizer.learning_rate,m._lr)\n",
        "#     print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, m._model.optimizer.learning_rate))\n",
        "#     # Run the loop for this epoch in the training mode\n",
        "#     train_perplexity = run_one_epoch(m, train_data,is_training=True,verbose=True)\n",
        "#     print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
        "        \n",
        "#     # Run the loop for this epoch in the validation mode\n",
        "#     valid_perplexity = run_one_epoch(m, valid_data,is_training=False,verbose=False)\n",
        "#     print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
        "    \n",
        "# # Run the loop in the testing mode to see how effective was our training\n",
        "# test_perplexity = run_one_epoch(m, test_data,is_training=False,verbose=False)\n",
        "# print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
        "\n"
      ],
      "metadata": {
        "id": "UNfE4GHw57Wg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGDPspOI5-Ny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}