{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SolpN41gSOKO"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai it is $5 each.\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuKQd7L3S7RL",
        "outputId": "1b4812e7-f9d3-4964-dfad-dbb86ab96ad6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "Strange\n",
            "loves\n",
            "pav\n",
            "bhaji\n",
            "of\n",
            "mumbai\n",
            "it\n",
            "is\n",
            "$\n",
            "5\n",
            "each\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('''\"Let's go to N.Y.!\"''')\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGYubTUNTanv",
        "outputId": "b8494b18-b434-41c7-e1cd-be892d19878d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "Let\n",
            "'s\n",
            "go\n",
            "to\n",
            "N.Y.\n",
            "!\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq4OXYWYV0Jr",
        "outputId": "1d32a76e-df8a-4a8c-dff5-c10baf5ed744"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHB1r8ebWEvs",
        "outputId": "5bd6ca15-9167-40de-f4c3-2cfee41306df"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Defaults',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_components',\n",
              " '_config',\n",
              " '_disabled',\n",
              " '_ensure_doc',\n",
              " '_ensure_doc_with_context',\n",
              " '_factory_meta',\n",
              " '_get_pipe_index',\n",
              " '_has_gpu_model',\n",
              " '_link_components',\n",
              " '_meta',\n",
              " '_multiprocessing_pipe',\n",
              " '_optimizer',\n",
              " '_path',\n",
              " '_pipe_configs',\n",
              " '_pipe_meta',\n",
              " '_resolve_component_status',\n",
              " 'add_pipe',\n",
              " 'analyze_pipes',\n",
              " 'batch_size',\n",
              " 'begin_training',\n",
              " 'component',\n",
              " 'component_names',\n",
              " 'components',\n",
              " 'config',\n",
              " 'create_optimizer',\n",
              " 'create_pipe',\n",
              " 'create_pipe_from_source',\n",
              " 'default_config',\n",
              " 'default_error_handler',\n",
              " 'disable_pipe',\n",
              " 'disable_pipes',\n",
              " 'disabled',\n",
              " 'enable_pipe',\n",
              " 'evaluate',\n",
              " 'factories',\n",
              " 'factory',\n",
              " 'factory_names',\n",
              " 'from_bytes',\n",
              " 'from_config',\n",
              " 'from_disk',\n",
              " 'get_factory_meta',\n",
              " 'get_factory_name',\n",
              " 'get_pipe',\n",
              " 'get_pipe_config',\n",
              " 'get_pipe_meta',\n",
              " 'has_factory',\n",
              " 'has_pipe',\n",
              " 'initialize',\n",
              " 'lang',\n",
              " 'make_doc',\n",
              " 'max_length',\n",
              " 'meta',\n",
              " 'path',\n",
              " 'pipe',\n",
              " 'pipe_factories',\n",
              " 'pipe_labels',\n",
              " 'pipe_names',\n",
              " 'pipeline',\n",
              " 'rehearse',\n",
              " 'remove_pipe',\n",
              " 'rename_pipe',\n",
              " 'replace_listeners',\n",
              " 'replace_pipe',\n",
              " 'resume_training',\n",
              " 'select_pipes',\n",
              " 'set_error_handler',\n",
              " 'set_factory_meta',\n",
              " 'to_bytes',\n",
              " 'to_disk',\n",
              " 'tokenizer',\n",
              " 'update',\n",
              " 'use_params',\n",
              " 'vocab']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHcTwskDWFiA",
        "outputId": "ccb599cd-d4dc-4e47-995e-d4751909b3bd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token0 = doc[0]\n",
        "token0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZLPU0RDWJH2",
        "outputId": "5814bd7a-05da-451c-a2f4-111820974c6d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dr."
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(token0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VR3WoncW0aA",
        "outputId": "0bba5882-8b1d-42bd-d656-9066f0c98301"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_dep',\n",
              " 'has_extension',\n",
              " 'has_head',\n",
              " 'has_morph',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'iob_strings',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'set_morph',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token0.is_alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r43_rxMaW444",
        "outputId": "abec4d9a-a680-4e1e-cb48-559785127c98"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tony gave me $ dollars\")\n",
        "\n",
        "token2 = doc[2]\n",
        "token2.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5lkqUrRrXEwk",
        "outputId": "3438b3f7-a2b3-4881-c5dc-d65cdb529906"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'me'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token2.like_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXPGr-r3XWhw",
        "outputId": "96af7e6b-a822-4c25-c456-714e8197bf54"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3 = doc[3]\n",
        "token3.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NFANdaJtXb1A",
        "outputId": "a21a6753-41b0-41c7-e8c5-108e71bc95ba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3.is_currency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj23H9LBXhTL",
        "outputId": "bd172960-3e40-4d00-aee2-837d64166fd8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(f\"{token} ==> index: {token.i},\\n\\t is_alpha: {token.is_alpha}, \\n\\t is_punct: {token.is_punct}, \\n\\t like_num: {token.like_num}, \\n\\t is_currency: {token.is_currency}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syHTQRRiXklw",
        "outputId": "aa49f47a-37ad-45d8-9667-dcd5bb6eedd1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tony ==> index: 0,\n",
            "\t is_alpha: True, \n",
            "\t is_punct: False, \n",
            "\t like_num: False, \n",
            "\t is_currency: False\n",
            "\n",
            "gave ==> index: 1,\n",
            "\t is_alpha: True, \n",
            "\t is_punct: False, \n",
            "\t like_num: False, \n",
            "\t is_currency: False\n",
            "\n",
            "me ==> index: 2,\n",
            "\t is_alpha: True, \n",
            "\t is_punct: False, \n",
            "\t like_num: False, \n",
            "\t is_currency: False\n",
            "\n",
            "$ ==> index: 3,\n",
            "\t is_alpha: False, \n",
            "\t is_punct: False, \n",
            "\t like_num: False, \n",
            "\t is_currency: True\n",
            "\n",
            "dollars ==> index: 4,\n",
            "\t is_alpha: True, \n",
            "\t is_punct: False, \n",
            "\t like_num: False, \n",
            "\t is_currency: False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"students.txt\") as f:\n",
        "#   text = f.readlines()\n",
        "# text\n",
        "\n",
        "# text = ' '.join(text)\n",
        "\n",
        "# doc = nlp(text)\n",
        "# emails = []\n",
        "# for token in doc:\n",
        "#   if token.like_email:\n",
        "#     emails.append(token.text)\n",
        "# emails"
      ],
      "metadata": {
        "id": "MIO1juchYPW9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"tl\")\n",
        "\n",
        "doc = nlp(\"Kumusta ka, maayos kalang ba? pahingi naman ₱5000.00\")\n",
        "for token in doc:\n",
        "  print(token, token.is_currency, token.like_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQFADcRLcuIo",
        "outputId": "8f7d1066-0ea5-4bde-bbcc-93a2c191a29c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kumusta False False\n",
            "ka False False\n",
            ", False False\n",
            "maayos False False\n",
            "kalang False False\n",
            "ba False False\n",
            "? False False\n",
            "pahingi False False\n",
            "naman False False\n",
            "₱ True False\n",
            "5000.00 False True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "\n",
        "tokens = [token.text for token in doc]"
      ],
      "metadata": {
        "id": "R-Cx7UXOdieZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0tCxd9jg83c",
        "outputId": "81d82f20-1200-4ea9-80f7-70d674c6edde"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.symbols import ORTH\n",
        "\n",
        "nlp.tokenizer.add_special_case(\"gimme\", [\n",
        "    {ORTH: \"gim\"},\n",
        "    {ORTH: \"me\"}\n",
        "])\n",
        "\n",
        "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlPp0IQ_g93z",
        "outputId": "fdcd818d-9e0f-4fc5-bdb4-b08e251d9f96"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbaw. Hulk loves chat of delhi\")\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "1s3HzYl_hVeU",
        "outputId": "c0470818-b277-4062-94e8-8e7a191fa582"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-91a31c25152e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr. Strange loves pav bhaji of mumbaw. Hulk loves chat of delhi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGVPl3FPh4HQ",
        "outputId": "da676576-e568-4035-c476-a7f052aa54a8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe('sentencizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDlcBD-_hue1",
        "outputId": "cbc392c8-a366-4eaa-ad48-1dd8d00c85f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7afde7c07780>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqXNbtglh576",
        "outputId": "f20b15da-92f4-4416-b4c2-7a29858a39f6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sentencizer']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Dr. Strange loves pav bhaji of mumbaw. Hulk loves chat of delhi\")\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8y2kpBOh7iE",
        "outputId": "b64a44e0-7a39-4a6e-e362-33abb3f375e0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.\n",
            "Strange loves pav bhaji of mumbaw.\n",
            "Hulk loves chat of delhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise\n",
        "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
        "\n",
        "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
      ],
      "metadata": {
        "id": "SmsvNZTCjF_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "'''\n",
        "\n",
        "# TODO: Write code here\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(text)\n",
        "website_ds = [token.text for token in doc if token.like_url]\n",
        "website_ds\n",
        "\n",
        "\n",
        "# Hint: token has an attribute that can be used to detect a url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RufEv_7kifxw",
        "outputId": "adab1101-1902-4f8a-a11f-dc1a439b413a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://www.data.gov/',\n",
              " 'http://www.science',\n",
              " 'http://data.gov.uk/.',\n",
              " 'http://www3.norc.org/gss+website/',\n",
              " 'http://www.europeansocialsurvey.org/.']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Extract all money transaction from below sentence along with currency. Output should be,\n",
        "\n",
        "two $\n",
        "\n",
        "500 €"
      ],
      "metadata": {
        "id": "IQGmy_cSjEuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(transactions)\n",
        "ans = []\n",
        "for i in range(len(doc)):\n",
        "  if i == 12:\n",
        "    break\n",
        "  if doc[i].like_num & doc[i+1].is_currency:\n",
        "    ans.append(f\"{doc[i]} {doc[i+1]}\")\n",
        "\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nKyZg43jDvR",
        "outputId": "0e5eed18-0614-48ff-974b-0ba0dcfe8bd9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['two $', '500 €']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = nlp(\"two\")\n",
        "for token in temp:\n",
        "  print(token, token.like_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvjS3tnek_XI",
        "outputId": "bcc47177-d54b-49db-9065-79f2e448e0f8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer key"
      ],
      "metadata": {
        "id": "mImw7McMm1xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spacy Tokenizer Exercise Solution\n",
        "# Collecting dataset websites from a book paragraph\n",
        "# Collecting Data Websites From Think Stats Book Paragraph\n",
        "\n",
        "# https://greenteapress.com/thinkstats2/thinkstats2.pdf\n",
        "\n",
        "# text='''\n",
        "# Look for data to help you address the question. Governments are good\n",
        "# sources because data from public research is often freely available. Good\n",
        "# places to start include http://www.data.gov/, and http://www.science.\n",
        "# gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "# Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n",
        "# and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "# '''\n",
        "# doc = nlp(text)\n",
        "# data_websites = [token.text for token in doc if token.like_url ]\n",
        "# data_websites\n",
        "# ['http://www.data.gov/',\n",
        "#  'http://www.science',\n",
        "#  'http://data.gov.uk/.',\n",
        "#  'http://www3.norc.org/gss+website/',\n",
        "#  'http://www.europeansocialsurvey.org/.']\n",
        "# Figure out all transactions from this text with amount and currency\n",
        "# transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "# doc = nlp(transactions)\n",
        "# for token in doc:\n",
        "#     if token.like_num and doc[token.i+1].is_currency:\n",
        "#         print(token.text, doc[token.i+1].text)\n",
        "# two $\n",
        "# 500 €"
      ],
      "metadata": {
        "id": "dlZhfjKwlU_n"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nS8CoMNm2fa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}